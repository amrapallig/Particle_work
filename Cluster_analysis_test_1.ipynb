{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AG_ cluster initialization for diffusivity calculations\n",
    "import numpy as np\n",
    "import netCDF4\n",
    "import matplotlib.pyplot as plt\n",
    "from cartopy import config\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from scipy import stats\n",
    "import time\n",
    "#import numexpr as ne\n",
    "import xarray as xr\n",
    "rEarth = 6371220. #in m ##  get from file variable #f_in.sphere_radius\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#find clusters from initial file\n",
    "# select density layer global data\n",
    "path='/global/cscratch1/sd/garanaik/e3sm_scratch/cori-knl/'\n",
    "den=1026 # input which layer required for filter\n",
    "\n",
    "path_out=\"./gulfstream/d_1026_c_1d_r100_4/\"\n",
    "\n",
    "path_ini='/global/cscratch1/sd/garanaik/data/'\n",
    "data_pt_ini = xr.open_dataset(path_ini+'particles_17011_18to6_16000_dt30min_test_41nb_jan_jul_culled.nc')\n",
    "itemindex = np.where(data_pt_ini.buoyancyParticle[0,:]==den)\n",
    "n=itemindex[0]\n",
    "np1=(n[0]);npm1=(n[-1]) # min and max index for particular buoyancy layer\n",
    "index=np.arange(np1,npm1) #index of all particles belonging to density layer\n",
    "#print(index.shape)  \n",
    "\n",
    "data_den_ini=data_pt_ini.sel(nParticles=slice((n[0]),(n[-1])))  \n",
    "\n",
    "\n",
    "llon=data_den_ini.lonParticle[:,:].T\n",
    "llat=data_den_ini.latParticle[:,:].T\n",
    "\n",
    "\n",
    "### cluster grid points # either selected domain or global\n",
    "#x,y=np.meshgrid(np.linspace(-90,-20,71),np.linspace(20,50,31)) #cluster center at 1 degree\n",
    "x,y=np.meshgrid(np.linspace(-90,-20,71),np.linspace(20,50,31))  #test \n",
    "x=np.ravel(np.deg2rad(x)); y=np.ravel(np.deg2rad(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster, circular\n",
    "radius=100000# 100km= 100,000 m\n",
    "import time\n",
    "t1 = time.time()\n",
    "radiustree = radius / (rEarth * np.sin(np.maximum(np.abs(np.min(y)),np.abs(np.max(y)))))  #y is lat, x is long\n",
    "#radiustree = radius / (rEarth * np.sin(np.abs(45)))\n",
    "#print(time.time()-t1)\n",
    "#print(radiustree)\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "from scipy.spatial import cKDTree as KDTree\n",
    "allparticles = KDTree(np.vstack((llon[:,0],llat[:,0])).T) #llon[:,0] nparticles,time\n",
    "#print(time.time()-t1)\n",
    "search = KDTree(np.vstack((x,y)).T)                       # x,y cluster centers  defined...\n",
    "#print(time.time()-t1)\n",
    "clusters = search.query_ball_tree(allparticles, radiustree)\n",
    "#print(time.time()-t1) \n",
    "\n",
    "\n",
    "Nclusters = x.ravel().shape[0]\n",
    "#Nclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lat lon in degree\n",
    "def latlon_from_xyz(xp,yp,zp,r=rEarth):\n",
    "    rinv=1/r \n",
    "    #plat=np.rad2deg(np.arcsin(zp/ np.sqrt(xp**2 + yp**2 + zp**2)))\n",
    "    plat=(np.rad2deg(np.arcsin(zp*rinv)))\n",
    "    plon=(np.rad2deg(np.arctan2(yp, xp)))\n",
    "    return plat, plon\n",
    "\n",
    "def normalized_haversine_formula(phi1, phi2, lam1, lam2,r=rEarth):\n",
    "    \n",
    "    #phi2=np.deg2rad(phi2)\n",
    "    #phi1=np.deg2rad(phi1)\n",
    "    #lam1=np.deg2rad(lam1)\n",
    "    #lam2=np.deg2rad(lam2)\n",
    "    \n",
    "    dphi =( phi2 - phi1)\n",
    "    dlam = (lam2 - lam1)\n",
    "\n",
    "    a = np.sin(dphi/2.0)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlam/2.0)**2\n",
    "    c = r*2.0 * np.arctan2(np.sqrt(a), np.sqrt(1.0-a))\n",
    "\n",
    "    return c\n",
    "\n",
    "def spherical_bearing(phi1, phi2, lam1, lam2): \n",
    "     \n",
    "    #phi2=np.deg2rad(phi2)\n",
    "    #phi1=np.deg2rad(phi1)\n",
    "    #lam1=np.deg2rad(lam1)\n",
    "    #lam2=np.deg2rad(lam2)\n",
    "    \n",
    "    dphi = (phi2 - phi1)\n",
    "    dlam = (lam2 - lam1)\n",
    "\n",
    "    return np.arctan2(np.sin(dlam)*np.cos(phi2), np.cos(phi1)*np.sin(phi2) - np.sin(phi1)*np.cos(phi2)*np.cos(dlam)) #}}}\n",
    "\n",
    "def signed_distances(phi1, phi2, lam1, lam2, r=rEarth):  #{{{\n",
    "  \n",
    "    dx = normalized_haversine_formula(phi1, phi1, lam1, lam2,r )\n",
    "    dy = normalized_haversine_formula(phi1, phi2, lam1, lam1,r )\n",
    "    # fix orientation of points\n",
    "    bearing = spherical_bearing(phi1, phi2, lam1, lam2)\n",
    "    # because arctan2 returns results from -pi to pi for bearing, flip values to get right sign\n",
    "    dx -= 2*dx*(bearing < 0)\n",
    "    dy -= 2*dy*(abs(bearing) > np.pi/2.0)\n",
    "    ux = dx/(24.*60.*60.*1)  #m/s\n",
    "    uy = dy/(24.*60.*60.*1)  #m/s\n",
    "#    ux = dx/(24.*60.*60.*2)  #m/s\n",
    "#    uy = dy/(24.*60.*60.*2)  #m/s\n",
    "   \n",
    "    return ux, uy #}}}  #velocity with dt 1day\n",
    "\n",
    "\n",
    "def bootstrap_ci(data,rep):\n",
    "    n=len(data)\n",
    "    xb = np.random.choice(data, (n, rep), replace=True)\n",
    "    yb = 1/np.arange(1, n+1)[:, None] * np.cumsum(xb, axis=0)\n",
    "    upper, lower = np.percentile(yb, [2.5, 97.5], axis=1)\n",
    "    \n",
    "    \n",
    "    return np.nanmean(upper),np.nanmean(lower),np.nanmean(yb)\n",
    " \n",
    "def cluster_mean_dispersion(plat,plon,r):   #here plat, plon corresponds to that of all \n",
    "                                             #particles in one cluster, one realization, one time step, one layer\n",
    "    clat=np.nanmean(plat)\n",
    "    clon=np.nanmean(plon)\n",
    "    \n",
    "    \n",
    "    dx = normalized_haversine_formula(clat, clat, clon, plon, r)\n",
    "    dy = normalized_haversine_formula(clat, plat, clon, clon, r)\n",
    "    dr = normalized_haversine_formula(clat, plat, clon, plon, r)\n",
    "    \n",
    "    bearing = spherical_bearing(clat, plat, clon, plon)\n",
    "    dx -= 2*dx*(bearing < 0)\n",
    "    dy -= 2*dy*(np.fabs(bearing) > np.pi/2.0)\n",
    "    \n",
    "    dxdx_sum = np.sum(dx*dx)/(len(plat)-1)\n",
    "    dydy_sum = np.sum(dy*dy)/(len(plat)-1)\n",
    "    dxdy_sum = np.sum(dx*dy)/(len(plat)-1)\n",
    "    drdr_sum = np.sum(dr*dr)/(len(plat)-1)\n",
    "    \n",
    "    return clon,clat,dxdx_sum,dydy_sum,dxdy_sum,drdr_sum,len(plat) \n",
    "\n",
    "def corr(u):\n",
    "    rr= np.zeros((len(u[:,0]),len(u[0,:])))\n",
    "    for i in np.arange(len(u[:,0])):\n",
    "        uu=u[i,:]\n",
    "        r=(np.correlate(uu,uu,mode=\"full\"))\n",
    "        r = r[r.size//2:]\n",
    "    rr[i,:]=r\n",
    "        #print(r.shape)\n",
    "    return np.nanmean(rr,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all realizations_ all particles_to den layer\n",
    "#each file corresponds to each realization of 6 month data\n",
    "# this part can be combined to one dataset with all realzations, time, particles.\n",
    "# For now, I have kept the realizations fies separate\n",
    "\n",
    "#input file read\n",
    "path='/global/cscratch1/sd/garanaik/e3sm_scratch/cori-knl/'\n",
    "data=xr.open_mfdataset(path+\n",
    "   r'E3SM_pio2_one-year_test_oRRS18to6v3_pt_16000_4096_4096_256_Ldt30min_41nb_culled_withrestart_from81yr/'\n",
    "   r'run/analysis_members/*.nc',concat_dim=\"Time\",combine='nested')\n",
    "#data corresponding to density layer\n",
    "data_den=data.sel(nParticles=slice((n[0]),(n[-1])),Time=slice(0,180)) \n",
    "\n",
    "#convert to lat and long\n",
    "import time\n",
    "t1 = time.time()\n",
    "plat,plon=latlon_from_xyz(data_den.xParticle.values,data_den.yParticle.values,data_den.zParticle.values)\n",
    "z=data_den.zLevelParticle.values\n",
    "print(time.time() - t1)  #88.12878680229187\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lat, lon depth from input file, all time\n",
    "plat_r=np.deg2rad(plat)\n",
    "plon_r=np.deg2rad(plon)\n",
    "llon_allt=plon_r.T\n",
    "llat_allt=plat_r.T\n",
    "zt=z.T\n",
    "\n",
    "# size of cluster, partcles, time\n",
    "Ntime=data_den.xParticle[:,0].shape[0]\n",
    "Nparticles=data_den.xParticle[0,:].shape[0]\n",
    "Nclusters = x.ravel().shape[0]\n",
    "\n",
    "#start of cluster analysis\n",
    "\n",
    "mux      = np.zeros((Ntime,Nclusters))\n",
    "muy      = np.zeros((Ntime,Nclusters))\n",
    "dxdx     = np.zeros((Ntime,Nclusters))\n",
    "dydy     = np.zeros((Ntime,Nclusters))\n",
    "dxdy     = np.zeros((Ntime,Nclusters))\n",
    "drdr     = np.zeros((Ntime,Nclusters))\n",
    "Npart    = np.zeros((Ntime,Nclusters))\n",
    "depth    = np.zeros((Ntime,Nclusters))\n",
    "urms     = np.zeros((Ntime-1,Nclusters))\n",
    "vrms     = np.zeros((Ntime-1,Nclusters))\n",
    "umean    = np.zeros((Ntime-1,Nclusters))\n",
    "vmean    = np.zeros((Ntime-1,Nclusters))\n",
    "rx    = np.zeros((Ntime-1,Nclusters))\n",
    "ry    = np.zeros((Ntime-1,Nclusters))\n",
    "\n",
    "t1 = time.time()\n",
    "for c in np.arange(Nclusters):\n",
    "    t2 = time.time()\n",
    "    ind=clusters[c]\n",
    "    #print(len(ind))\n",
    "    if (len(ind)>0):\n",
    "        \n",
    "        uvel,vvel   = signed_distances(plat_r[:-1,ind], plat_r[1:,ind],plon_r[:-1,ind], plon_r[1:,ind])\n",
    "        u_allt=uvel[:,:].T\n",
    "        v_allt=vvel[:,:].T\n",
    "        #print(u_allt.shape) #ind, time\n",
    "    \n",
    "        p_u=u_allt-np.nanmean(u_allt,axis=0)\n",
    "        p_v=v_allt-np.nanmean(v_allt,axis=0)\n",
    "        #print(p_u.shape)\n",
    "        rrx = corr(p_u)\n",
    "        rry = corr(p_v)\n",
    "        rx[:,c]=rrx\n",
    "        ry[:,c]=rry\n",
    "        urms[:,c]=np.nanstd(p_u)\n",
    "        vrms[:,c]=np.nanstd(p_v)\n",
    "        umean[:,c]=np.nanmean(p_u)\n",
    "        vmean[:,c]=np.nanmean(p_v)  \n",
    "        for t in np.arange(Ntime-1):\n",
    "            p_lat=llat_allt[ind,t]\n",
    "            p_lon=llon_allt[ind,t]\n",
    "            mux[t,c],muy[t,c], dxdx[t,c],dydy[t,c],dxdy[t,c],drdr[t,c], Npart[t,c]=cluster_mean_dispersion(p_lat,p_lon,r=rEarth)\n",
    "                  \n",
    "    \n",
    "    #print(c,\"time per cluster=\",time.time()-t2)#5,0.1249542236328125, 844,0.22299627304077148\n",
    "print(time,\"time per time=\",time.time()-t1)    \n",
    "\n",
    "\n",
    "# save each realization cluster data\n",
    "#import _pickle as pickle\n",
    "#pickle.dump(mux,open(path_out+\"mux_81.p\",\"wb\"))\n",
    "#pickle.dump(muy,open(path_out+\"muy_81.p\",\"wb\"))\n",
    "#pickle.dump(dxdx,open(path_out+\"dxdx_81.p\",\"wb\"))\n",
    "#pickle.dump(dydy,open(path_out+\"dydy_81.p\",\"wb\"))\n",
    "#pickle.dump(dxdy,open(path_out+\"dxdy_81.p\",\"wb\"))\n",
    "#pickle.dump(drdr,open(path_out+\"drdr_81.p\",\"wb\"))\n",
    "#pickle.dump(Npart,open(path_out+\"Npart_81.p\",\"wb\"))\n",
    "#pickle.dump(depth,open(path_out+\"depth_81.p\",\"wb\"))\n",
    "#pickle.dump(urms,open(path_out+\"urms_81.p\",\"wb\"))\n",
    "#pickle.dump(vrms,open(path_out+\"vrms_81.p\",\"wb\"))\n",
    "#pickle.dump(umean,open(path_out+\"umean_81.p\",\"wb\"))\n",
    "#pickle.dump(vmean,open(path_out+\"vmean_81.p\",\"wb\"))   \n",
    "#pickle.dump(rx,open(path_out+\"rx_81.p\",\"wb\"))\n",
    "#pickle.dump(ry,open(path_out+\"ry_81.p\",\"wb\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx_r=((np.nanmean(rx,axis=1))/np.nanmean(rx[0,:]))\n",
    "ry_r=((np.nanmean(ry,axis=1))/np.nanmean(ry[0,:]))\n",
    "fig= plt.figure(figsize=(5,6))\n",
    "plt.subplot(311)\n",
    "plt.plot(rx_r,'r')\n",
    "plt.plot(ry_r,'b')\n",
    "plt.ylabel('R')\n",
    "plt.plot([0,180],[0,0],':k')\n",
    "\n",
    "plt.subplot(312)\n",
    "diffx=np.nanmean(rx[0,:])*np.cumsum(rx_r)*24*3600\n",
    "diffy=np.nanmean(ry[0,:])*np.cumsum(ry_r)*24*3600\n",
    "plt.plot(diffx,'r')\n",
    "plt.plot(diffy,'b')\n",
    "plt.legend(['x','y'])\n",
    "plt.ylabel('diff_int')\n",
    "\n",
    "plt.subplot(313)\n",
    "diffx=0.5*(np.diff(dxdx,axis=0))/(3600*24*2)\n",
    "difx=np.nanmean(diffx,axis=1)\n",
    "diffy=0.5*(np.diff(dydy,axis=0))/(3600*24*2)\n",
    "dify=np.nanmean(diffy,axis=1)\n",
    "plt.plot(difx[:-4],'r')\n",
    "plt.plot(dify[:-4],'b')\n",
    "plt.xlabel('days')\n",
    "plt.ylabel('diff_disp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
